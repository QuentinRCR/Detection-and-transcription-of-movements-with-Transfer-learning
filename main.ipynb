{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directML = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast  # Used to convert string representation of a list to a list\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import seaborn as sns\n",
    "from colour import Color\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# Import necessary paths\n",
    "absolute_root_path = os.path.abspath('').replace(\"\\\\\",\"/\") + \"/..\"\n",
    "sys.path.insert(1,absolute_root_path)\n",
    "from paths import preprocessed_datasets_path, save_weights_path,EMNIST_path\n",
    "\n",
    "if not directML:\n",
    "    from keras.models import Sequential, load_model, save_model\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization,Dropout,LSTM,Masking, Bidirectional\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger,Callback\n",
    "    from keras import backend as K # to be able to change the learning rate on tge fly\n",
    "else:\n",
    "    from tensorflow.keras import backend as K # to be able to change the learning rate on tge fly\n",
    "    from tensorflow.keras.models import Sequential, load_model, save_model\n",
    "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization,Dropout,LSTM,Masking, Bidirectional\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger,Callback "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = np.array([\"DigiLeTs\"]) #DigiLeTs or EMNIST or [DigiLeTs, BRUSH] or [DigiLeTs, EMNIST]\n",
    "network_type = np.array([\"LSTM\"]) # LSTM or CNN\n",
    "training_mode = \"Fine-tuned\" # \"Normal\" or \"Fine-tuned\"\n",
    "image_shape = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the distribution with a maximum number of classes\n",
    "def return_subset(len_each_class, X,Y):\n",
    "    unique_classes = np.unique(Y)\n",
    "\n",
    "    selected_indices = []\n",
    "\n",
    "    # Iterate over each unique class\n",
    "    for class_label in unique_classes:\n",
    "        # Find indices of examples belonging to the current class\n",
    "        class_indices = np.where(Y == class_label)[0]\n",
    "        \n",
    "        # Select the first len_each_class examples for the current class\n",
    "        selected_indices.extend(class_indices[:len_each_class])\n",
    "\n",
    "    # Use the selected indices to extract the corresponding examples\n",
    "    selected_data = X[selected_indices]\n",
    "    selected_labels = Y[selected_indices]\n",
    "\n",
    "    return selected_data,selected_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow for the use of an additional validation data during the training steps \n",
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) not in [3, 4]:\n",
    "                raise ValueError()\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) == 3:\n",
    "                validation_data, validation_targets, validation_set_name = validation_set\n",
    "                sample_weights = None\n",
    "            elif len(validation_set) == 4:\n",
    "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            results = self.model.evaluate(x=validation_data,\n",
    "                                          y=validation_targets,\n",
    "                                          verbose=self.verbose,\n",
    "                                          sample_weight=sample_weights,\n",
    "                                          batch_size=self.batch_size)\n",
    "\n",
    "            for metric, result in zip(self.model.metrics_names,results):\n",
    "                valuename = validation_set_name + '_' + metric\n",
    "                self.history.setdefault(valuename, []).append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the letter corresponding to index\n",
    "def get_letter_from_index(index):\n",
    "    return chr(EMNIST_df_mapping.loc[index]['lc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the index corresponding to letter\n",
    "def get_index_from_letter(letter):\n",
    "    return EMNIST_df_mapping[EMNIST_df_mapping['lc']==ord(letter)].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a scatter with colorful output and that remove filling data (ie: [-10,-10])\n",
    "def draw_points(X,Y):\n",
    "    X= np.delete(X, np.where(X == -10))\n",
    "    Y= np.delete(Y, np.where(Y == -10))\n",
    "\n",
    "    color_gradient = Color(\"green\").range_to(Color(\"red\"),len(X))\n",
    "    color_gradient = [color.hex for color in color_gradient]\n",
    "\n",
    "    plt.plot(X,Y,color=\"k\", label=\"Linear interpolation\")\n",
    "    plt.scatter(X,Y,c=color_gradient,label=\"Points\")\n",
    "    plt.title(f'Points: {X.shape[0]}')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From an image, find the corresponding initial data point from the target dataset and plot then\n",
    "def plot_original(image):\n",
    "    flattened_array= image.flatten()\n",
    "    correct_row = None\n",
    "    for index, row in target_df.iterrows():\n",
    "        # Check if the current row is equal to the flattened array\n",
    "        if np.array_equal(row['images'], flattened_array):\n",
    "            print(f\"Arrays are equal at index {index}\")\n",
    "            correct_row = row\n",
    "            break\n",
    "\n",
    "        plt.figure()\n",
    "        draw_points(correct_row['X'],correct_row['Y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the image corresponding to the provided list of X values\n",
    "def get_image_from_X(X,df):\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if the current row is equal to the flattened array\n",
    "        if np.array_equal(row['X'], X):\n",
    "            print(f\"Arrays are equal at index {index}\")\n",
    "            return row['images'].reshape(image_shape,image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the specified try\n",
    "def load_try(try_name):\n",
    "    # get the name of the model used for the try\n",
    "    with open(f'{save_weights_path}/{try_name}/model_type.txt', newline='') as csvfile:\n",
    "        model_name = csvfile.readline()\n",
    "\n",
    "    # load the model\n",
    "    model = load_model(f'{save_weights_path}/initialization/{model_name}.h5')\n",
    "\n",
    "    # load weights from the try\n",
    "    model.load_weights(f'{save_weights_path}/{try_name}/data.h5')\n",
    "\n",
    "    print(f'Loading model {model_name} for try {try_name}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a moving average without modifying the value that cannot be averaged\n",
    "def moving_average(a,smooth_value=5):\n",
    "    moving_average = np.convolve(a,np.ones(smooth_value)/smooth_value,mode='valid') # do the average\n",
    "    moving_average = np.append(moving_average,a[-(smooth_value//2):]) # add points at the beginning that can't be smoothed\n",
    "    moving_average = np.insert(moving_average,0,a[:smooth_value//2]) # add points at the end that can't be smoothed\n",
    "    return moving_average\n",
    "\n",
    "# smooth the provided xy coordinates\n",
    "def smoothen_points(xy_shift, smooth_value):\n",
    "    x,y = xy_shift.T\n",
    "    x,y = moving_average(x,smooth_value),moving_average(y,smooth_value)\n",
    "\n",
    "    return np.array([x,y]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load EMNIST df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the mapping between the index and asci value of characters\n",
    "dataframe_type  = \"emnist-byclass\"\n",
    "EMNIST_df_mapping = pd.read_csv(f'{EMNIST_path}/{dataframe_type}-mapping.txt',header=None,delimiter=\" \",index_col=0,names=[\"lc\"])\n",
    "\n",
    "# get the indices corresponding to the lowercase characters from the alphabet\n",
    "index_lowercase = list(EMNIST_df_mapping[EMNIST_df_mapping[\"lc\"].isin([97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"EMNIST\",source_dataset): # to avoid loading too much into memory\n",
    "    # read EMNIST_df with figures, lowercase, upercase\n",
    "    EMNIST_df= pd.read_csv(f'{EMNIST_path}/{dataframe_type}-train.csv',header=None, dtype=np.uint8)\n",
    "\n",
    "    # keep only the lowercase letters\n",
    "    EMNIST_df = EMNIST_df[EMNIST_df[0].isin(index_lowercase)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a random sample of the dataset\n",
    "if np.isin(\"EMNIST\",source_dataset):\n",
    "    row = EMNIST_df.iloc[np.random.randint(0,EMNIST_df.shape[0])]\n",
    "\n",
    "    image = np.flip(np.rot90(row.values[1:].reshape((image_shape,image_shape)),axes=(1,0)),axis=1)\n",
    "    plt.imshow(image,\"gray\")\n",
    "    plt.title(get_letter_from_index(row.values[0]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load digilets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"DigiLeTs\",source_dataset): # to avoid loading too much into memory\n",
    "    digilet_df = pd.read_csv(f'{preprocessed_datasets_path}/digilets-inter3-28-21.csv')\n",
    "    \n",
    "    # transform only the necessary features for the type of network to avoid overloading the memory\n",
    "    if np.isin(\"CNN\",network_type):\n",
    "        digilet_df['images'] = digilet_df['images'].apply(ast.literal_eval)\n",
    "        digilet_df['images'] = digilet_df['images'].apply(np.array)\n",
    "    if np.isin(\"LSTM\",network_type):\n",
    "        digilet_df['X'] = digilet_df['X'].apply(lambda x: x.replace(\"nan\",\"-10\")) # replace nan with a defined value to be able to eliminate them\n",
    "        digilet_df['X'] = digilet_df['X'].apply(eval)\n",
    "        digilet_df['X'] = digilet_df['X'].apply(np.array)\n",
    "        digilet_df['Y'] = digilet_df['Y'].apply(lambda x: x.replace(\"nan\",\"-10\")) # replace nan with a defined value to be able to eliminate them\n",
    "        digilet_df['Y'] = digilet_df['Y'].apply(eval)\n",
    "        digilet_df['Y'] = digilet_df['Y'].apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a sample of the data\n",
    "if np.isin(\"DigiLeTs\",source_dataset) and np.isin(\"CNN\",network_type):\n",
    "    # look at the data\n",
    "    row = digilet_df.iloc[np.random.randint(0,digilet_df.shape[0])]\n",
    "\n",
    "    plt.imshow(row[\"images\"].reshape((image_shape,image_shape)),\"gray\")\n",
    "    plt.title(row[\"letter\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BRUSH dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"BRUSH\",source_dataset): # to avoid loading too much into memory\n",
    "    brush_df = pd.read_csv(f'{preprocessed_datasets_path}/BRUSH-inter3--.csv')\n",
    "    \n",
    "    # transform only the necessary features for the type of network to avoid overloading the memory\n",
    "    if np.isin(\"CNN\",network_type):\n",
    "        brush_df['images'] = brush_df['images'].apply(ast.literal_eval)\n",
    "        brush_df['images'] = brush_df['images'].apply(np.array)\n",
    "    if np.isin(\"LSTM\",network_type):\n",
    "        brush_df['X'] = brush_df['X'].apply(lambda x: x.replace(\"nan\",\"-10\")) # replace nan with a defined value to be able to eliminate them\n",
    "        brush_df['X'] = brush_df['X'].apply(eval)\n",
    "        brush_df['X'] = brush_df['X'].apply(np.array)\n",
    "        brush_df['Y'] = brush_df['Y'].apply(lambda x: x.replace(\"nan\",\"-10\")) # replace nan with a defined value to be able to eliminate them\n",
    "        brush_df['Y'] = brush_df['Y'].apply(eval)\n",
    "        brush_df['Y'] = brush_df['Y'].apply(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the target dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.read_csv(f'{preprocessed_datasets_path}/multi_user-inter3-28-21.csv')\n",
    "interpolate = True\n",
    "\n",
    "# Apply only the necessary transformation to avoid overloading the memory\n",
    "if np.isin(\"CNN\",network_type):\n",
    "    target_df['images'] = target_df['images'].apply(ast.literal_eval)\n",
    "    target_df['images'] = target_df['images'].apply(np.array)\n",
    "if np.isin(\"LSTM\",network_type):\n",
    "    target_df['X'] = target_df['X'].apply(ast.literal_eval)\n",
    "    target_df['X'] = target_df['X'].apply(np.array)\n",
    "    target_df['Y'] = target_df['Y'].apply(ast.literal_eval)\n",
    "    target_df['Y'] = target_df['Y'].apply(np.array)\n",
    "    # if the data is not interpolated (ie: is not the same length) pad to 32 to have an even length\n",
    "    if not interpolate:\n",
    "        target_df['X'] = target_df['X'].apply(lambda x: np.pad(x, (0, 32 - len(x)), constant_values=-10)) \n",
    "        target_df['Y'] = target_df['Y'].apply(lambda y: np.pad(y, (0, 32 - len(y)), constant_values=-10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data\n",
    "if np.isin(\"CNN\",network_type):\n",
    "    row = target_df.iloc[np.random.randint(0,target_df.shape[0])]\n",
    "    image = row['images'].reshape((image_shape,image_shape))\n",
    "    plt.imshow(image,\"gray\")\n",
    "    plt.title(row['letter'])\n",
    "    plt.show()\n",
    "    # plot_original(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the EMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the DigiLeTs dataset in the correct format for the neural network\n",
    "def get_EMNIST_X_Y(max_number_for_each_letter,random_seed):\n",
    "    EMNIST_X = ((EMNIST_df.values[:,1:]).reshape(EMNIST_df.shape[0],image_shape,image_shape))\n",
    "    \n",
    "    # flip upside down\n",
    "    EMNIST_X = np.rot90(EMNIST_X, k=1, axes=(2,1))\n",
    "    EMNIST_X= np.flip(EMNIST_X,axis=2)\n",
    "\n",
    "    EMNIST_Y= EMNIST_df.values[:,0]\n",
    "\n",
    "    # A shuffling is required to be done since only the first few examples are taken into account\n",
    "    EMNIST_X, EMNIST_Y = return_subset(max_number_for_each_letter,EMNIST_X,EMNIST_Y)\n",
    "    EMNIST_X, EMNIST_Y = shuffle(EMNIST_X, EMNIST_Y,random_state=random_seed)\n",
    "    return  EMNIST_X, EMNIST_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Digilets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the DigiLeTs dataset in the correct format for the neural network\n",
    "def get_DigiLeTs_X_Y():\n",
    "    DigiLeTs_Y = digilet_df['letter'].apply(get_index_from_letter).values # convert letters to numbers\n",
    "    \n",
    "    if np.isin(\"CNN\",network_type):\n",
    "        DigiLeTs_X = np.vstack(digilet_df['images'].values).reshape((digilet_df.shape[0],image_shape,image_shape))\n",
    "    if np.isin(\"LSTM\",network_type):\n",
    "        DigiLeTs_X = np.array([np.vstack(digilet_df['X'].values), np.vstack(digilet_df['Y'].values)]).transpose(1,2,0) # reshape the data to fit the LSTM\n",
    "\n",
    "    return DigiLeTs_X,DigiLeTs_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare BRUSH dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the BRUSH dataset in the correct format for the neural network\n",
    "def get_BRUSH_X_Y():\n",
    "    BRUSH_Y = brush_df['letter'].apply(get_index_from_letter).values # convert letters to numbers\n",
    "    \n",
    "    if np.isin(\"CNN\",network_type):\n",
    "        BRUSH_X = np.vstack(brush_df['images'].values).reshape((brush_df.shape[0],image_shape,image_shape))\n",
    "    if np.isin(\"LSTM\",network_type):\n",
    "        BRUSH_X = np.array([np.vstack(brush_df['X'].values), np.vstack(brush_df['Y'].values)]).transpose(1,2,0) # reshape the data to fit the LSTM\n",
    "\n",
    "    return BRUSH_X,BRUSH_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the target dataset in the correct format for the neural network\n",
    "# If the person to exclude is None, it returns a train, dev(200), test(200) and same(200) distribution split\n",
    "# If the person to exclude is defined it returns a train, dev(99), test(1) with the dev containing data from the excluded person\n",
    "def get_target_X_Y(DigiLeTs_shape, person_to_exclude):\n",
    "    target_df_subset = target_df[target_df['name']!=person_to_exclude]\n",
    "\n",
    "    target_Y = target_df_subset['letter'].apply(get_index_from_letter).values # convert letters to numbers\n",
    "    if np.isin(\"CNN\",network_type):\n",
    "        target_X = np.vstack(target_df_subset['images'].values).reshape((target_df_subset.shape[0],image_shape,image_shape))\n",
    "    if np.isin(\"LSTM\",network_type):\n",
    "        target_X = np.array([np.vstack(target_df_subset['X'].values), np.vstack(target_df_subset['Y'].values)]).transpose(1,2,0) # reshape the data to fit the LSTM\n",
    "\n",
    "        # pad the LSTM value so that it has the same shape that the DigiLeTs values\n",
    "        target_X = np.pad(target_X, ((0, 0), (0, DigiLeTs_shape-target_X.shape[1]), (0, 0)), constant_values=[-10,-10])\n",
    "\n",
    "    if np.isin(person_to_exclude,target_df['name']): # test if the person the exclude exist\n",
    "        excluded_person_df = target_df[target_df['name']==person_to_exclude]\n",
    "        excluded_Y = excluded_person_df['letter'].apply(get_index_from_letter).values # convert letters to numbers\n",
    "        if np.isin(\"CNN\",network_type):\n",
    "            excluded_X = np.vstack(excluded_person_df['images'].values).reshape((excluded_person_df.shape[0],image_shape,image_shape))\n",
    "        if np.isin(\"LSTM\",network_type):\n",
    "            excluded_X = np.array([np.vstack(excluded_person_df['X'].values), np.vstack(excluded_person_df['Y'].values)]).transpose(1,2,0) # reshape the data to fit the LSTM\n",
    "            excluded_X = np.pad(excluded_X, ((0, 0), (0, DigiLeTs_shape-excluded_X.shape[1]), (0, 0)), constant_values=[-10,-10])\n",
    "    else:\n",
    "        excluded_X, excluded_Y = None, None\n",
    "\n",
    "    return (target_X,target_Y,excluded_X,excluded_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split in train, dev and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_dev_test_dataset(test_dev_df_size,EMNIST_data_multiplier,random_seed,person_to_exclude=None):\n",
    "    person_independent_mode = np.isin(person_to_exclude,target_df['name'])\n",
    "    if person_to_exclude is not None and not person_independent_mode:\n",
    "        raise ValueError(\"Wrong name for person independent\")\n",
    "\n",
    "    # Initialise every variables\n",
    "    train_X, test_same_dist_X, train_Y_categorical, test_same_dist_Y_categorical,source_X_train, source_X_test, target_X_train, source_Y_train_categorical, source_Y_test_categorical, target_Y_train_categorical, target_Y_train,source_Y_test,source_Y_train,train_Y,test_same_dist_Y = None,None,None,None,None,None,None,None,None,None,None,None,None,None,None\n",
    "    \n",
    "    # take the correct number of examples from the EMNIST. Use one sample per letter (26 letter) when not EMNIST data is required to avoid breaking the slip \n",
    "    number_examples_from_source =26 if EMNIST_data_multiplier==0 else (target_df.shape[0] - 2*test_dev_df_size)*EMNIST_data_multiplier\n",
    "\n",
    "    # set the correct dataset and the correct mode\n",
    "    if np.isin(\"BRUSH\",source_dataset) and np.isin(\"DigiLeTs\",source_dataset):\n",
    "        source_X_D,source_Y_D = get_DigiLeTs_X_Y()\n",
    "        source_X_B,source_Y_B = get_BRUSH_X_Y()\n",
    "        source_X,source_Y = np.concatenate((source_X_D,source_X_B)),np.concatenate((source_Y_D,source_Y_B))\n",
    "    elif np.isin(\"EMNIST\",source_dataset) and np.isin(\"DigiLeTs\",source_dataset):\n",
    "        source_X_D,source_Y_D = get_DigiLeTs_X_Y()\n",
    "        source_X_E,source_Y_E = get_EMNIST_X_Y(max_number_for_each_letter=2000,random_seed = random_seed)\n",
    "        # add in priority the digilets dataset and then the EMNIST dataset and keep only what is going to be kept at the end\n",
    "        source_X,source_Y = np.concatenate((source_X_D,source_X_E))[:number_examples_from_source+26],np.concatenate((source_Y_D,source_Y_E))[:number_examples_from_source+26]\n",
    "    elif np.isin(\"DigiLeTs\",source_dataset):\n",
    "        source_X,source_Y = get_DigiLeTs_X_Y()\n",
    "    elif np.isin(\"EMNIST\",source_dataset):\n",
    "        source_X,source_Y = get_EMNIST_X_Y(max_number_for_each_letter=2000,random_seed =random_seed)\n",
    "\n",
    "    # Shuffling required because sometimes datasets are just concatenated\n",
    "    source_X, source_Y = shuffle(source_X, source_Y,random_state=random_seed)\n",
    "\n",
    "    target_X, target_Y, excluded_X, excluded_Y = get_target_X_Y(DigiLeTs_shape= source_X.shape[1], person_to_exclude = person_to_exclude)\n",
    "\n",
    "    # create a relatively small dev and test set (to be able to detect 0.5% improvement) to add as many example as possible in the train set\n",
    "    # stratify the datasets to be able to have a correct representation of all the class in the test/dev set\n",
    "    if not person_independent_mode:\n",
    "        target_X_remaining, test_X, target_Y_remaining, test_Y = train_test_split(target_X,target_Y,random_state=random_seed,stratify=target_Y,test_size=test_dev_df_size)\n",
    "        target_X_remaining, dev_X, target_Y_remaining, dev_Y = train_test_split(target_X_remaining,target_Y_remaining,random_state=random_seed,stratify=target_Y_remaining,test_size=test_dev_df_size)\n",
    "    else:\n",
    "        target_X_remaining, target_Y_remaining = target_X, target_Y\n",
    "        dev_X, dev_Y = shuffle(excluded_X[:-1], excluded_Y[:-1],random_state=random_seed)\n",
    "        test_X, test_Y = shuffle(np.expand_dims(excluded_X[-1], axis=0), np.expand_dims(excluded_Y[-1], axis=0),random_state=random_seed)\n",
    "\n",
    "    # get a fraction of the source data that is stratified\n",
    "    _ , source_X_fraction, _, source_Y_fraction =  train_test_split(source_X,source_Y,random_state=random_seed,test_size=number_examples_from_source, stratify=source_Y)\n",
    "\n",
    "    # shift labels so that they are between 0 and 25\n",
    "    test_Y_categorical = to_categorical(test_Y-36, num_classes=26)\n",
    "    dev_Y_categorical = to_categorical(dev_Y-36, num_classes=26)\n",
    "\n",
    "    # for normal training\n",
    "    if training_mode==\"Normal\":\n",
    "        train_X = np.concatenate((source_X_fraction,target_X_remaining))\n",
    "        train_Y = np.concatenate((source_Y_fraction,target_Y_remaining))\n",
    "        if not person_independent_mode:\n",
    "            train_X, test_same_dist_X ,train_Y, test_same_dist_Y = train_test_split(train_X,train_Y,random_state=random_seed,test_size=test_dev_df_size,stratify=train_Y)\n",
    "            test_same_dist_Y_categorical = to_categorical(test_same_dist_Y-36, num_classes=26)\n",
    "        else:\n",
    "            train_X,train_Y = shuffle(train_X,train_Y,random_state=random_seed)\n",
    "\n",
    "        train_Y_categorical = to_categorical(train_Y-36, num_classes=26)\n",
    "\n",
    "    # for fine tuning\n",
    "    elif training_mode==\"Fine-tuned\":\n",
    "        source_X_train, source_X_test, source_Y_train, source_Y_test = train_test_split(source_X_fraction,source_Y_fraction,stratify=source_Y_fraction, test_size=test_dev_df_size,random_state=random_seed)\n",
    "        target_X_train, target_Y_train = target_X_remaining, target_Y_remaining\n",
    "\n",
    "        source_Y_train_categorical = to_categorical(source_Y_train-36, num_classes=26)\n",
    "        source_Y_test_categorical = to_categorical(source_Y_test-36, num_classes=26)\n",
    "        target_Y_train_categorical = to_categorical(target_Y_train-36, num_classes=26)\n",
    "\n",
    "    # pack the data to be returned\n",
    "    data_normal_training = (train_X, test_same_dist_X, dev_X, test_X, train_Y_categorical, test_same_dist_Y_categorical, dev_Y_categorical, test_Y_categorical)\n",
    "    data_fine_tuning = (source_X_train, source_X_test, target_X_train, dev_X, test_X, source_Y_train_categorical, source_Y_test_categorical, target_Y_train_categorical, dev_Y_categorical, test_Y_categorical)\n",
    "    accessory = (train_Y,test_same_dist_Y,source_Y_train,source_Y_test,target_Y_train,dev_Y,test_Y)\n",
    "    return (data_normal_training,data_fine_tuning,accessory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of querying the data\n",
    "data_normal_training,data_fine_tuning,accessory = split_train_dev_test_dataset(test_dev_df_size=200,EMNIST_data_multiplier=8,random_seed=42, person_to_exclude=None)\n",
    "\n",
    "# unpack\n",
    "train_X, test_same_dist_X, dev_X, test_X, train_Y_categorical, test_same_dist_Y_categorical, dev_Y_categorical, test_Y_categorical = data_normal_training\n",
    "source_X_train, source_X_test, target_X_train, dev_X, test_X, source_Y_train_categorical, source_Y_test_categorical, target_Y_train_categorical, dev_Y_categorical, test_Y_categorical = data_fine_tuning\n",
    "train_Y,test_same_dist_Y,source_Y_train,source_Y_test,target_Y_train, dev_Y,test_Y = accessory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model1 = Sequential()\n",
    "    model1.add(Conv2D(32, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model1.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dense(128, activation='relu'))\n",
    "    model1.add(Dense(26, activation='softmax'))\n",
    "    model1.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 for different shapes\n",
    "if np.isin(\"CNN\",network_type):\n",
    "    model1_1 = Sequential()\n",
    "    model1_1.add(Conv2D(32, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model1_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model1_1.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model1_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model1_1.add(Flatten())\n",
    "    model1_1.add(Dense(128, activation='relu'))\n",
    "    model1_1.add(Dense(26, activation='softmax'))\n",
    "    model1_1.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model1_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 but with old version of tensorflow to be able to use the GPU with direct ML\n",
    "if np.isin(\"CNN\",network_type):\n",
    "    model1_2 = Sequential()\n",
    "    model1_2.add(Conv2D(32, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model1_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model1_2.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model1_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model1_2.add(Flatten())\n",
    "    model1_2.add(Dense(128, activation='relu'))\n",
    "    model1_2.add(Dense(26, activation='softmax'))\n",
    "    model1_2.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model1_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model2 = Sequential()\n",
    "    model2.add(Conv2D(32, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model2.add(BatchNormalization())\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model2.add(BatchNormalization())\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Flatten())\n",
    "    model2.add(Dense(128, activation='relu'))\n",
    "    model2.add(BatchNormalization())\n",
    "    model2.add(Dense(26, activation='softmax'))\n",
    "    model2.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model3 = Sequential()\n",
    "    model3.add(Conv2D(32, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model3.add(BatchNormalization())\n",
    "    model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model3.add(Dropout(0.5))\n",
    "    model3.add(Flatten())\n",
    "    model3.add(Dense(128, activation='relu'))\n",
    "    model3.add(BatchNormalization())\n",
    "    model3.add(Dropout(0.5))\n",
    "    model3.add(Dense(26, activation='softmax'))\n",
    "    model3.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler model 1\n",
    "if np.isin(\"CNN\",network_type):\n",
    "    model4 = Sequential()\n",
    "    model4.add(Conv2D(32, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model4.add(Dropout(0.2))\n",
    "    model4.add(Flatten())\n",
    "    model4.add(Dense(128, activation='relu'))\n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(Dropout(0.2))\n",
    "    model4.add(Dense(26, activation='softmax'))\n",
    "    model4.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model5 = Sequential()\n",
    "    model5.add(Conv2D(32, kernel_size=(5, 5), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model5.add(Conv2D(64, kernel_size=(5, 5), activation='relu'))\n",
    "    model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model5.add(Flatten())\n",
    "    model5.add(Dense(128, activation='relu'))\n",
    "    model5.add(Dense(26, activation='softmax'))\n",
    "    model5.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model6 = Sequential()\n",
    "    model6.add(Conv2D(16, kernel_size=(5, 5), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model6.add(Flatten())\n",
    "    model6.add(Dense(64, activation='relu'))\n",
    "    model6.add(Dense(26, activation='softmax'))\n",
    "    model6.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model7 = Sequential()\n",
    "    model7.add(Conv2D(8, kernel_size=(5, 5), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model7.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model7.add(Flatten())\n",
    "    model7.add(Dense(26, activation='softmax'))\n",
    "    model7.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model7.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model8 = Sequential()\n",
    "    model8.add(Conv2D(4, kernel_size=(5, 5), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model8.add(Flatten())\n",
    "    model8.add(Dense(26, activation='softmax'))\n",
    "    model8.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model8.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model9 = Sequential()\n",
    "    model9.add(Conv2D(4, kernel_size=(5, 5), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model9.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model9.add(Dropout(0.25))  # Adjust the dropout rate as needed\n",
    "    model9.add(Flatten())\n",
    "    model9.add(Dense(26, activation='softmax'))\n",
    "    model9.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model9.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to add a second convolutional layer to have a chance of capturing info in several levels\n",
    "if np.isin(\"CNN\",network_type):\n",
    "    model10 = Sequential()\n",
    "    model10.add(Conv2D(4, kernel_size=(5, 5), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model10.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model10.add(Dropout(0.25))  # Adjust the dropout rate as needed\n",
    "    model10.add(Conv2D(4, kernel_size=(5, 5), activation='relu'))\n",
    "    model10.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model10.add(Dropout(0.25))  # Adjust the dropout rate as needed\n",
    "    model10.add(Flatten())\n",
    "    model10.add(Dense(26, activation='softmax'))\n",
    "    model10.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model10.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model11 = Sequential()\n",
    "    model11.add(Conv2D(4, kernel_size=(5, 5), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model11.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model11.add(Conv2D(4, kernel_size=(5, 5), activation='relu'))\n",
    "    model11.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model11.add(Flatten())\n",
    "    model11.add(Dense(26, activation='softmax'))\n",
    "    model11.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model11.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model12 = Sequential()\n",
    "    model12.add(Conv2D(16, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model12.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model12.add(Dropout(0.20))\n",
    "    model12.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "    model12.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model12.add(Flatten())\n",
    "    model12.add(Dense(26, activation='softmax'))\n",
    "    model12.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model12.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model13 = Sequential()\n",
    "    model13.add(Conv2D(16, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model13.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model13.add(Dropout(0.20))\n",
    "    model13.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "    model13.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model13.add(Flatten())\n",
    "    model13.add(Dense(26, activation='softmax'))\n",
    "    model13.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model13.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model14 = Sequential()\n",
    "    model14.add(Conv2D(16, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model14.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model14.add(Dropout(0.20))\n",
    "    model14.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "    model14.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model14.add(Flatten())\n",
    "    model14.add(Dense(128, activation='relu'))\n",
    "    model14.add(Dense(26, activation='softmax'))\n",
    "    model14.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model14.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 15 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same LSTM model that had great results during the protech\n",
    "if np.isin(\"LSTM\",network_type):\n",
    "    model15 = Sequential()\n",
    "    model15.add(LSTM(50, return_sequences=True, input_shape=(dev_X.shape[1], dev_X.shape[2])))\n",
    "    model15.add(Dropout(0.2))\n",
    "    model15.add(LSTM(50, return_sequences=False))\n",
    "    model15.add(Dropout(0.2))\n",
    "    model15.add(Dense(26, activation='softmax'))\n",
    "\n",
    "    model15.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model15.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 16 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"LSTM\",network_type):\n",
    "    model16 = Sequential()\n",
    "    model16.add(Masking(mask_value = [-10,-10], input_shape=(dev_X.shape[1], dev_X.shape[2])))\n",
    "    model16.add(LSTM(50))\n",
    "    model16.add(Dense(26, activation='softmax'))\n",
    "\n",
    "    model16.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 17 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"LSTM\",network_type):\n",
    "    model17 = Sequential()\n",
    "    model17.add(Masking(mask_value = [-10,-10], input_shape=(dev_X.shape[1], dev_X.shape[2])))\n",
    "    model17.add(Bidirectional(LSTM(50)))\n",
    "    model17.add(Dense(26, activation='softmax'))\n",
    "    model17.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model17.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different shape of 16\n",
    "if np.isin(\"LSTM\",network_type):\n",
    "    model18 = Sequential()\n",
    "    model18.add(Masking(mask_value = [-10,-10], input_shape=(dev_X.shape[1], dev_X.shape[2])))\n",
    "    model18.add(LSTM(50))\n",
    "    model18.add(Dense(26, activation='softmax'))\n",
    "    model18.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model18.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same 17 but with different shapes\n",
    "if np.isin(\"LSTM\",network_type):\n",
    "    model19 = Sequential()\n",
    "    model19.add(Masking(mask_value = [-10,-10], input_shape=(dev_X.shape[1], dev_X.shape[2])))\n",
    "    model19.add(Bidirectional(LSTM(50)))\n",
    "    model19.add(Dense(26, activation='softmax'))\n",
    "    model19.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model19.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model20 = Sequential()\n",
    "    model20.add(Conv2D(32, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model20.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model20.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model20.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model20.add(Flatten())\n",
    "    model20.add(Dense(128, activation='relu'))\n",
    "    model20.add(Dropout(0.25))\n",
    "    model20.add(Dense(26, activation='softmax'))\n",
    "    model20.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model20.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model21 = Sequential()\n",
    "    model21.add(Conv2D(32, strides=3, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model21.add(Conv2D(64, strides=3, kernel_size=(3, 3), activation='relu'))\n",
    "    model21.add(Flatten())\n",
    "    model21.add(Dense(128, activation='relu'))\n",
    "    model21.add(Dropout(0.25))\n",
    "    model21.add(Dense(26, activation='softmax'))\n",
    "    model21.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model21.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model22 = Sequential()\n",
    "    model22.add(Conv2D(32, strides=2, kernel_size=(3, 3), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model22.add(Conv2D(64, strides=2, kernel_size=(3, 3), activation='relu'))\n",
    "    model22.add(Flatten())\n",
    "    model22.add(Dense(128, activation='relu'))\n",
    "    model22.add(Dropout(0.25))\n",
    "    model22.add(Dense(26, activation='softmax'))\n",
    "    model22.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model22.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    model23 = Sequential()\n",
    "    model23.add(Conv2D(32, strides=3, kernel_size=(7, 7), input_shape=(image_shape, image_shape, 1), activation='relu'))\n",
    "    model23.add(Conv2D(64, strides=3, kernel_size=(7,7), activation='relu'))\n",
    "    model23.add(Flatten())\n",
    "    model23.add(Dense(128, activation='relu'))\n",
    "    model23.add(Dropout(0.25))\n",
    "    model23.add(Dense(26, activation='softmax'))\n",
    "    model23.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model23.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same model than what was used in the protech\n",
    "if np.isin(\"LSTM\",network_type):\n",
    "    model24 = Sequential()\n",
    "    model24.add(LSTM(50, return_sequences=True, input_shape=(dev_X.shape[1], dev_X.shape[2])))\n",
    "    model24.add(Dropout(0.2))\n",
    "    model24.add(LSTM(50, return_sequences=False))\n",
    "    model24.add(Dropout(0.2))\n",
    "    model24.add(Dense(26, activation='softmax'))\n",
    "    model24.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model24.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same 19 but with a dropout\n",
    "if np.isin(\"LSTM\",network_type):\n",
    "    model25 = Sequential()\n",
    "    model25.add(Masking(mask_value = [-10,-10], input_shape=(dev_X.shape[1], dev_X.shape[2])))\n",
    "    model25.add(Bidirectional(LSTM(50)))\n",
    "    model25.add(Dropout(0.2)) # to improve generalization\n",
    "    model25.add(Dense(26, activation='softmax'))\n",
    "    model25.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model25.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same 19 but with more units\n",
    "if np.isin(\"LSTM\",network_type):\n",
    "    model26 = Sequential()\n",
    "    model26.add(Masking(mask_value = [-10,-10], input_shape=(dev_X.shape[1], dev_X.shape[2])))\n",
    "    model26.add(Bidirectional(LSTM(100)))\n",
    "    model26.add(Dense(26, activation='softmax'))\n",
    "    model26.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model26.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same 19 but with less units\n",
    "if np.isin(\"LSTM\",network_type):\n",
    "    model27 = Sequential()\n",
    "    model27.add(Masking(mask_value = [-10,-10], input_shape=(dev_X.shape[1], dev_X.shape[2])))\n",
    "    model27.add(Bidirectional(LSTM(25)))\n",
    "    model27.add(Dense(26, activation='softmax'))\n",
    "    model27.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model27.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_mode==\"Normal\" and not directML:\n",
    "    max_epochs = 200\n",
    "    model_name = \"model1\"\n",
    "    try_name = '52-CNN'\n",
    "    patience = 30\n",
    "\n",
    "    folder_to_save = f'{save_weights_path}/{try_name}'\n",
    "\n",
    "    history = AdditionalValidationSets([(test_same_dist_X, test_same_dist_Y_categorical, 'test_same_dist')])\n",
    "\n",
    "    # if the model doesn't improve on the validation data accuracy for 3 epochs, stop the training\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=patience, restore_best_weights=True, mode='max')\n",
    "\n",
    "    # Save model with best validation accuracy\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        folder_to_save+'/data.h5',  # Specify the file to save the weights\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    csv_logger = CSVLogger(folder_to_save + \"/training_history.csv\")  # Save the history has soon as it is computed\n",
    "\n",
    "    # load the model if it already exist and save the new one if it doesn't already exist\n",
    "    # this allows to always have the same weights and make sure they are reset for a new training\n",
    "    model_initialization_path = f'{save_weights_path}/initialization/{model_name}.h5'\n",
    "    if model_name == \"modelTest\": # do not save the model when quickly iterating\n",
    "        model = eval(model_name)\n",
    "    elif not os.path.isfile(model_initialization_path):\n",
    "        model = eval(model_name) # load the model declared above\n",
    "        save_model(model,model_initialization_path)\n",
    "        print(\"Saving \"+model_name)\n",
    "    else:\n",
    "        if(input(\"This MODEL already exist, do you want to overwrite it ? (y/n)\")=='y'):\n",
    "            model = eval(model_name) # load the model declared above\n",
    "            save_model(model,model_initialization_path)\n",
    "            print(\"Saving \"+model_name)\n",
    "        else:\n",
    "            model = load_model(model_initialization_path)\n",
    "            print(\"Loading \",model_name)\n",
    "\n",
    "    overwrite = True\n",
    "    # create the folder saving this try  if it doesn't already exist\n",
    "    if not os.path.isdir(folder_to_save):\n",
    "        os.mkdir(folder_to_save) # create the folder\n",
    "    else:\n",
    "        overwrite = input(\"This folder already exist, do you want to overwrite it ? (y/n)\")=='y'\n",
    "\n",
    "    if overwrite:\n",
    "        with open(f'{folder_to_save}/model_type.txt', 'w') as f:\n",
    "            f.write(model_name)\n",
    "\n",
    "        model.fit(\n",
    "            train_X, train_Y_categorical,\n",
    "            epochs=max_epochs,\n",
    "            batch_size=32,\n",
    "            validation_data=(dev_X, dev_Y_categorical),\n",
    "            callbacks=[checkpoint, csv_logger, early_stopping,history]\n",
    "        )\n",
    "        history = history.history #keep only the useful part\n",
    "        pd.DataFrame(history).to_csv(folder_to_save + \"/training_history.csv\") # save the history containing the 2nd validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same than the old but with old version of tensorflow to be able to use the GPU with direct ML\n",
    "if training_mode==\"Normal\" and directML:\n",
    "    max_epochs = 200\n",
    "    model_name = \"model23\"\n",
    "    try_name = '56-CNN'\n",
    "    patience = 30\n",
    "\n",
    "    folder_to_save = f'{save_weights_path}/{try_name}'\n",
    "\n",
    "    a,b,c = train_X.shape\n",
    "    history = AdditionalValidationSets([(test_same_dist_X.reshape(test_same_dist_X.shape[0],b,c,1), test_same_dist_Y_categorical, 'test_same_dist')])\n",
    "\n",
    "    # if the model doesn't improve on the validation data accuracy for 3 epochs, stop the training\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', patience=patience, restore_best_weights=True, mode='max')\n",
    "\n",
    "    # Save model with best validation accuracy\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        folder_to_save+'/data.h5',  # Specify the file to save the weights\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    csv_logger = CSVLogger(folder_to_save + \"/training_history.csv\")  # Save the history has soon as it is computed\n",
    "\n",
    "    # load the model if it already exist and save the new one if it doesn't already exist\n",
    "    # this allows to always have the same weights and make sure they are reset for a new training\n",
    "    model_initialization_path = f'{save_weights_path}/initialization/{model_name}.h5'\n",
    "    if model_name == \"modelTest\": # do not save the model when quickly iterating\n",
    "        model = eval(model_name)\n",
    "    elif not os.path.isfile(model_initialization_path):\n",
    "        model = eval(model_name) # load the model declared above\n",
    "        save_model(model,model_initialization_path)\n",
    "        print(\"Saving \"+model_name)\n",
    "    else:\n",
    "        if(input(\"This MODEL already exist, do you want to overwrite it ? (y/n)\")=='y'):\n",
    "            model = eval(model_name) # load the model declared above\n",
    "            save_model(model,model_initialization_path)\n",
    "            print(\"Saving \"+model_name)\n",
    "        else:\n",
    "            model = load_model(model_initialization_path)\n",
    "            print(\"Loading \",model_name)\n",
    "\n",
    "    overwrite = True\n",
    "    # create the folder saving this try  if it doesn't already exist\n",
    "    if not os.path.isdir(folder_to_save):\n",
    "        os.mkdir(folder_to_save) # create the folder\n",
    "    else:\n",
    "        overwrite = input(\"This folder already exist, do you want to overwrite it ? (y/n)\")=='y'\n",
    "\n",
    "    if overwrite:\n",
    "        with open(f'{folder_to_save}/model_type.txt', 'w') as f:\n",
    "            f.write(model_name)\n",
    "\n",
    "        try:\n",
    "            model.fit(\n",
    "                train_X.reshape(a,b,c,1), train_Y_categorical,\n",
    "                epochs=max_epochs,\n",
    "                batch_size=32,\n",
    "                validation_data=(dev_X.reshape(dev_X.shape[0],b,c,1), dev_Y_categorical),\n",
    "                callbacks=[checkpoint, csv_logger, early_stopping,history]\n",
    "            )\n",
    "        finally:\n",
    "            history = history.history #keep only the useful part\n",
    "            pd.DataFrame(history).rename(columns={'acc':'accuracy','val_acc':'val_accuracy','test_same_dist_acc':'test_same_dist_accuracy'}).to_csv(folder_to_save + \"/training_history.csv\").to_csv(folder_to_save + \"/training_history.csv\") # save the history containing the 2nd validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with generic data and fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load desired dataset\n",
    "data_normal_training,data_fine_tuning,accessory = split_train_dev_test_dataset(test_dev_df_size=200,EMNIST_data_multiplier=8,random_seed=42, person_to_exclude=\"David\")\n",
    "source_X_train, source_X_test, target_X_train, dev_X, test_X, source_Y_train_categorical, source_Y_test_categorical, target_Y_train_categorical, dev_Y_categorical, test_Y_categorical = data_fine_tuning\n",
    "train_Y,test_same_dist_Y,source_Y_train,source_Y_test,target_Y_train, dev_Y,test_Y = accessory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_mode==\"Fine-tuned\" and not directML:\n",
    "    max_epochs = 500\n",
    "    model_name = \"model27\"\n",
    "    try_name = 'model27-LSTM'\n",
    "    patience = 30\n",
    "\n",
    "    folder_to_save = f'{save_weights_path}/{try_name}'\n",
    "\n",
    "    # if the model doesn't improve on the validation data accuracy for 3 epochs, stop the training\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=patience, restore_best_weights=True, mode='max')\n",
    "\n",
    "    # Save model with best validation accuracy\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        folder_to_save+'/data.h5',  # Specify the file to save the weights\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    csv_logger = CSVLogger(folder_to_save + \"/training_history.csv\")  # Save the history has soon as it is computed\n",
    "\n",
    "    # load the model if it already exist and save the new one if it doesn't already exist\n",
    "    # this allows to always have the same weights and make sure they are reset for a new training\n",
    "    model_initialization_path = f'{save_weights_path}/initialization/{model_name}.h5'\n",
    "    if not os.path.isfile(model_initialization_path):\n",
    "        model = eval(model_name) # load the model declared above\n",
    "        save_model(model,model_initialization_path)\n",
    "        print(\"Saving \"+model_name)\n",
    "    else:\n",
    "        model = load_model(model_initialization_path)\n",
    "        # model = load_try(\"model19-LSTM-BRUSH\") # uncomment to be able to continue training the model\n",
    "        print(\"Loading \",model_name)\n",
    "\n",
    "    overwrite = True\n",
    "    # create the folder saving this try  if it doesn't already exist\n",
    "    if not os.path.isdir(folder_to_save):\n",
    "        os.mkdir(folder_to_save) # create the folder\n",
    "    else:\n",
    "        overwrite = input(\"This folder already exist, do you want to overwrite it ? (y/n)\")=='y'\n",
    "\n",
    "    if overwrite:\n",
    "        with open(f'{folder_to_save}/model_type.txt', 'w') as f:\n",
    "            f.write(model_name)\n",
    "\n",
    "        history = model.fit(\n",
    "            source_X_train, source_Y_train_categorical,\n",
    "            epochs=max_epochs,\n",
    "            batch_size=32,\n",
    "            validation_data=(source_X_test, source_Y_test_categorical),\n",
    "            callbacks=[checkpoint,csv_logger, early_stopping]\n",
    "        )\n",
    "\n",
    "        history = history.history\n",
    "\n",
    "        pd.DataFrame(history).to_csv(folder_to_save + \"/training_history.csv\") # save the history containing the 2nd validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same than the old but with old version of tensorflow to be able to use the GPU with direct ML\n",
    "if training_mode==\"Fine-tuned\" and directML:\n",
    "    max_epochs = 500\n",
    "    model_name = \"model1_2\"\n",
    "    try_name = 'model1_2-CNN-Digi+EMINST-50'\n",
    "    patience = 30\n",
    "\n",
    "    a,b,c = source_X_train.shape\n",
    "    d,_,_ = source_X_test.shape\n",
    "\n",
    "    folder_to_save = f'{save_weights_path}/{try_name}'\n",
    "\n",
    "    # if the model doesn't improve on the validation data accuracy for 3 epochs, stop the training\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', patience=patience, restore_best_weights=True, mode='max')\n",
    "\n",
    "    # Save model with best validation accuracy\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        folder_to_save+'/data.h5',  # Specify the file to save the weights\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    csv_logger = CSVLogger(folder_to_save + \"/training_history.csv\")  # Save the history has soon as it is computed\n",
    "\n",
    "    # load the model if it already exist and save the new one if it doesn't already exist\n",
    "    # this allows to always have the same weights and make sure they are reset for a new training\n",
    "    model_initialization_path = f'{save_weights_path}/initialization/{model_name}.h5'\n",
    "    if not os.path.isfile(model_initialization_path):\n",
    "        model = eval(model_name) # load the model declared above\n",
    "        save_model(model,model_initialization_path)\n",
    "        print(\"Saving \"+model_name)\n",
    "    else:\n",
    "        model = load_model(model_initialization_path)\n",
    "        print(\"Loading \",model_name)\n",
    "\n",
    "    overwrite = True\n",
    "    # create the folder saving this try  if it doesn't already exist\n",
    "    if not os.path.isdir(folder_to_save):\n",
    "        os.mkdir(folder_to_save) # create the folder\n",
    "    else:\n",
    "        overwrite = input(\"This folder already exist, do you want to overwrite it ? (y/n)\")=='y'\n",
    "\n",
    "    if overwrite:\n",
    "        with open(f'{folder_to_save}/model_type.txt', 'w') as f:\n",
    "            f.write(model_name)\n",
    "\n",
    "        history = model.fit(\n",
    "            source_X_train.reshape(a,b,c,1), source_Y_train_categorical,\n",
    "            epochs=max_epochs,\n",
    "            batch_size=32,\n",
    "            validation_data=(source_X_test.reshape(d,b,c,1), source_Y_test_categorical),\n",
    "            callbacks=[checkpoint,csv_logger, early_stopping]\n",
    "        )\n",
    "\n",
    "        history = history.history\n",
    "\n",
    "        pd.DataFrame(history).rename(columns={'acc':'accuracy','val_acc':'val_accuracy','test_same_dist_acc':'test_same_dist_accuracy'}).rename(columns={'acc':'accuracy','val_acc':'val_accuracy','test_same_dist_acc':'test_same_dist_accuracy'}).to_csv(folder_to_save + \"/training_history.csv\") # save the history containing the 2nd validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune on target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune on CPU\n",
    "if training_mode==\"Fine-tuned\" and not directML:\n",
    "    max_epochs = 200\n",
    "    batch_size = 64\n",
    "    model_to_load = \"model1_2-CNN-Digi+EMINST\"\n",
    "    try_name = '68-CNN-FT'\n",
    "    patience = 30\n",
    "    learning_rate = 0.01 # default 0.001\n",
    "    number_try = 1\n",
    "    layer_to_freeze = [] #index of the layer to freeze\n",
    "    people_to_exclude = []\n",
    "\n",
    "    initial_model = model_to_load.split('-')[0]\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=patience, restore_best_weights=True, mode='max')\n",
    "    \n",
    "    wrong_name = np.any([not np.isin(name,np.unique(target_df['name'])) for name in people_to_exclude])\n",
    "    # if wrong_name:\n",
    "    #     print([not np.isin(name,np.unique(target_df['name'])) for name in people_to_exclude])\n",
    "    #     raise ValueError(\"Wrong person name\")\n",
    "    # if len(people_to_exclude)==0:\n",
    "    #     people_to_exclude = [None]\n",
    "\n",
    "    for person_to_exclude in people_to_exclude:\n",
    "        if person_to_exclude is not None:\n",
    "            custom_try_name = f'{try_name}-{person_to_exclude}'\n",
    "        else:\n",
    "            custom_try_name = try_name\n",
    "\n",
    "        # get the right dataset\n",
    "        data_normal_training,data_fine_tuning,accessory = split_train_dev_test_dataset(test_dev_df_size=200,EMNIST_data_multiplier=8,random_seed=42, person_to_exclude=person_to_exclude)\n",
    "        source_X_train, source_X_test, target_X_train, dev_X, test_X, source_Y_train_categorical, source_Y_test_categorical, target_Y_train_categorical, dev_Y_categorical, test_Y_categorical = data_fine_tuning\n",
    "\n",
    "        for try_number in range(number_try):\n",
    "            if number_try>1:\n",
    "                folder_to_save = f'{save_weights_path}/{custom_try_name}-{try_number}'\n",
    "            else:\n",
    "                folder_to_save = f'{save_weights_path}/{custom_try_name}'\n",
    "\n",
    "            # if the model doesn't improve on the validation data accuracy for 3 epochs, stop the training\n",
    "\n",
    "            # Save model with best validation accuracy\n",
    "            checkpoint = ModelCheckpoint(\n",
    "                folder_to_save+'/data.h5',  # Specify the file to save the weights\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                mode='max',\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            csv_logger = CSVLogger(folder_to_save + \"/training_history.csv\")  # Save the history has soon as it is computed\n",
    "\n",
    "            # load the model if it already exist and save the new one if it doesn't already exist\n",
    "            # this allows to always have the same weights and make sure they are reset for a new training\n",
    "            model_initialization_path = f'{save_weights_path}/{model_to_load}/data.h5'\n",
    "            if not os.path.isfile(model_initialization_path):\n",
    "                print(\"The model you are trying to load doesn't exist\")\n",
    "            else:\n",
    "                model = load_try(model_to_load)\n",
    "                print(\"Loading \",model_to_load)\n",
    "\n",
    "            if len(layer_to_freeze)!=0:\n",
    "                # freeze layers\n",
    "                for i,layer in enumerate(model.layers):\n",
    "                    if np.isin(i,layer_to_freeze):\n",
    "                        layer.trainable = False\n",
    "                        print(layer,\" is no longer trainable\")\n",
    "\n",
    "                # recompile so that the changes take into effect\n",
    "                model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "            # set the learning rate of the model\n",
    "            K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "\n",
    "            overwrite = True\n",
    "            # create the folder saving this try  if it doesn't already exist\n",
    "            if not os.path.isdir(folder_to_save):\n",
    "                os.mkdir(folder_to_save) # create the folder\n",
    "            else:\n",
    "                overwrite = input(\"This folder already exist, do you want to overwrite it ? (y/n)\")=='y'\n",
    "\n",
    "            if overwrite:\n",
    "                with open(f'{folder_to_save}/model_type.txt', 'w') as f:\n",
    "                    f.write(initial_model)\n",
    "\n",
    "                history = model.fit(\n",
    "                    target_X_train, target_Y_train_categorical,\n",
    "                    epochs=max_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(dev_X, dev_Y_categorical),\n",
    "                    callbacks=[checkpoint, csv_logger, early_stopping]\n",
    "                )\n",
    "\n",
    "                history = history.history #keep only the useful part\n",
    "                pd.DataFrame(history).to_csv(folder_to_save + \"/training_history.csv\") # save the history containing the 2nd validation dataset\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same than the old but with old version of tensorflow to be able to use the GPU with direct ML\n",
    "if training_mode==\"Fine-tuned\" and directML:\n",
    "    max_epochs = 300\n",
    "    batch_size = 32\n",
    "    model_to_load = \"model1_2-CNN-Digi+EMINST-50\"\n",
    "    try_name = '69-CNN-FT'\n",
    "    patience = 30\n",
    "    learning_rate = 0.001 # default 0.001\n",
    "    number_try = 1\n",
    "    layer_to_freeze = [] #index of the layer to freeze\n",
    "    people_to_exclude = []\n",
    "\n",
    "    initial_model = model_to_load.split('-')[0]\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', patience=patience, restore_best_weights=True, mode='max')\n",
    "    \n",
    "    wrong_name = np.any([not np.isin(name,np.unique(target_df['name'])) for name in people_to_exclude])\n",
    "    # if wrong_name:\n",
    "    #     print([not np.isin(name,np.unique(target_df['name'])) for name in people_to_exclude])\n",
    "    #     raise ValueError(\"Wrong person name\")\n",
    "    # if len(people_to_exclude)==0:\n",
    "    #     people_to_exclude = [None]\n",
    "\n",
    "    for person_to_exclude in people_to_exclude:\n",
    "        if person_to_exclude is not None:\n",
    "            custom_try_name = f'{try_name}-{person_to_exclude}'\n",
    "        else:\n",
    "            custom_try_name = try_name\n",
    "\n",
    "        # get the right dataset\n",
    "        data_normal_training,data_fine_tuning,accessory = split_train_dev_test_dataset(test_dev_df_size=200,EMNIST_data_multiplier=8,random_seed=42, person_to_exclude=person_to_exclude)\n",
    "        source_X_train, source_X_test, target_X_train, dev_X, test_X, source_Y_train_categorical, source_Y_test_categorical, target_Y_train_categorical, dev_Y_categorical, test_Y_categorical = data_fine_tuning\n",
    "\n",
    "        for try_number in range(number_try):\n",
    "            if number_try>1:\n",
    "                folder_to_save = f'{save_weights_path}/{custom_try_name}-{try_number}'\n",
    "            else:\n",
    "                folder_to_save = f'{save_weights_path}/{custom_try_name}'\n",
    "\n",
    "            # if the model doesn't improve on the validation data accuracy for 3 epochs, stop the training\n",
    "\n",
    "            # Save model with best validation accuracy\n",
    "            checkpoint = ModelCheckpoint(\n",
    "                folder_to_save+'/data.h5',  # Specify the file to save the weights\n",
    "                monitor='val_acc',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                mode='max',\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            csv_logger = CSVLogger(folder_to_save + \"/training_history.csv\")  # Save the history has soon as it is computed\n",
    "\n",
    "            # load the model if it already exist and save the new one if it doesn't already exist\n",
    "            # this allows to always have the same weights and make sure they are reset for a new training\n",
    "            model_initialization_path = f'{save_weights_path}/{model_to_load}/data.h5'\n",
    "            if not os.path.isfile(model_initialization_path):\n",
    "                raise ValueError(\"The model you are trying to load doesn't exist\")\n",
    "            else:\n",
    "                model = load_try(model_to_load)\n",
    "                print(\"Loading \",model_to_load)\n",
    "\n",
    "            if len(layer_to_freeze)!=0:\n",
    "                # freeze layers\n",
    "                for i,layer in enumerate(model.layers):\n",
    "                    if np.isin(i,layer_to_freeze):\n",
    "                        layer.trainable = False\n",
    "                        print(layer,\" is no longer trainable\")\n",
    "\n",
    "                # recompile so that the changes take into effect\n",
    "                model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "            # set the learning rate of the model\n",
    "            K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "\n",
    "            overwrite = True\n",
    "            # create the folder saving this try  if it doesn't already exist\n",
    "            if not os.path.isdir(folder_to_save):\n",
    "                os.mkdir(folder_to_save) # create the folder\n",
    "            else:\n",
    "                overwrite = input(\"This folder already exist, do you want to overwrite it ? (y/n)\")=='y'\n",
    "\n",
    "            if overwrite:\n",
    "                with open(f'{folder_to_save}/model_type.txt', 'w') as f:\n",
    "                    f.write(initial_model)\n",
    "\n",
    "                a,b,c = target_X_train.shape\n",
    "                d,_,_ = dev_X.shape\n",
    "\n",
    "                history = model.fit(\n",
    "                    target_X_train.reshape(a,b,c,1), target_Y_train_categorical,\n",
    "                    epochs=max_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(dev_X.reshape(d,b,c,1), dev_Y_categorical),\n",
    "                    callbacks=[checkpoint, csv_logger, early_stopping]\n",
    "                )\n",
    "\n",
    "                history = history.history #keep only the useful part\n",
    "                pd.DataFrame(history).rename(columns={'acc':'accuracy','val_acc':'val_accuracy','test_same_dist_acc':'test_same_dist_accuracy'}).to_csv(folder_to_save + \"/training_history.csv\") # save the history containing the 2nd validation dataset\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise SystemExit(\"Stop right there!\") # stop the run when running everything in one go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load history from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results by using the name of the model or a regex expression\n",
    "models_to_comparee = ['68-CNN-FT-[0-9]**']\n",
    "plot_training = True\n",
    "plot_dev = True\n",
    "plot_same_div = True if training_mode == \"Normal\" else False\n",
    "smoothened_plot = True\n",
    "smoothness_level = 10\n",
    "max_search_beginning = 0\n",
    "\n",
    "path_to_compare = np.array([])\n",
    "for models_to_compare in models_to_comparee:\n",
    "    path_to_compare = np.concatenate((path_to_compare,np.array(glob.glob(f'{save_weights_path}/{models_to_compare}'))))\n",
    "\n",
    "\n",
    "histories = [(pd.read_csv(f'{model_path}/training_history.csv'),model_path.split('/')[-1],(len(path_to_compare)-i)/len(path_to_compare)) for i,model_path in enumerate(path_to_compare)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for history,model_name,i in reversed(histories):\n",
    "    if plot_training:\n",
    "        plt.plot(history['loss'], label=f'{model_name}: Training Loss',c=(0,0,1,i))\n",
    "    if plot_dev:\n",
    "        plt.plot(history['val_loss'], label=f'{model_name}: Validation Loss',c=(0,0.5,0,i))\n",
    "    if plot_same_div:\n",
    "        plt.plot(history['test_same_dist_loss'], label=f'{model_name}: Same Distribution Loss',c=(0.75, 0.0, 0.75,i))\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "for history,model_name,i in reversed(histories):\n",
    "    if plot_training:\n",
    "        plt.plot(history['accuracy'], label=f'{model_name}: Training Accuracy',c=(0,0,1,i))\n",
    "    if plot_dev:\n",
    "        plt.plot(history['val_accuracy'], label=f'{model_name}: Validation Accuracy',c=(0,0.5,0,i))\n",
    "    if plot_same_div:\n",
    "        plt.plot(history['test_same_dist_accuracy'], label=f'{model_name}: Same Distribution Accuracy',c=(0.75, 0.0, 0.75,i))\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if smoothened_plot:\n",
    "    #smothered curve \n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for history,model_name,i in reversed(histories):\n",
    "        if plot_training:\n",
    "            plt.plot(moving_average(history['loss'],smoothness_level), label=f'{model_name}Training Loss',c=(0,0,1,i))\n",
    "        if plot_dev:\n",
    "            plt.plot(moving_average(history['val_loss'],smoothness_level), label=f'{model_name}Validation Loss',c=(0,0.5,0,i))\n",
    "        if plot_same_div:\n",
    "            plt.plot(moving_average(history['test_same_dist_loss'],smoothness_level), label=f'{model_name}Same Distribution Loss',c=(0.75, 0.0, 0.75,i))\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for history,model_name,i in reversed(histories):\n",
    "        if plot_training: \n",
    "            plt.plot(moving_average(history['accuracy'],smoothness_level), label=f'{model_name}Training Accuracy',c=(0,0,1,i))\n",
    "        if plot_dev: \n",
    "            plt.plot(moving_average(history['val_accuracy'],smoothness_level), label=f'{model_name}Validation Accuracy',c=(0,0.5,0,i))\n",
    "        if plot_same_div: \n",
    "            plt.plot(moving_average(history['test_same_dist_accuracy'],smoothness_level), label=f'{model_name}Same Distribution Accuracy',c=(0.75, 0.0, 0.75,i))\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "epoch_best_result = np.argmax(history['val_accuracy'][max_search_beginning:])+max_search_beginning\n",
    "print(f'Best result (for the 1st model): (for epoch {epoch_best_result+1})')\n",
    "print(\"Accuracy:\",history['accuracy'][epoch_best_result] ,\"\\nLoss:\",history['loss'][epoch_best_result],\"\\nVal_accuracy:\",history['val_accuracy'][epoch_best_result], \"\\nVal_loss\",history['val_loss'][epoch_best_result],\"\\nSame_dist_accuracy\",history['test_same_dist_accuracy'][epoch_best_result] if plot_same_div else \"\",\"\\nSame_dist_loss\",history['test_same_dist_loss'][epoch_best_result]  if plot_same_div else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the mean performance of the models matched by the regex expression\n",
    "models_name = '69-CNN-FT' #regex of the try name\n",
    "fine_tun= True\n",
    "\n",
    "train_acc = []\n",
    "val_acc= []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "same_dist_acc = []\n",
    "same_dist_loss = []\n",
    "try_paths = glob.glob(f'{save_weights_path}/{models_name}')\n",
    "\n",
    "for try_path in try_paths:\n",
    "    history =pd.read_csv(f'{try_path}/training_history.csv')\n",
    "\n",
    "    epoch_best_result = np.argmax(history['val_accuracy'])\n",
    "    train_acc.append(history['accuracy'][epoch_best_result])\n",
    "    train_loss.append(history['loss'][epoch_best_result])\n",
    "    val_acc.append(history['val_accuracy'][epoch_best_result])\n",
    "    val_loss.append(history['val_loss'][epoch_best_result])\n",
    "\n",
    "    if not fine_tun:\n",
    "        same_dist_acc.append(history['test_same_dist_accuracy'][epoch_best_result])\n",
    "        same_dist_loss.append(history['test_same_dist_loss'][epoch_best_result])\n",
    "\n",
    "print(\"Train acc\",np.mean(train_acc))\n",
    "print(\"Train loss\",np.mean(train_loss))\n",
    "print(\"Val acc\",np.mean(val_acc))\n",
    "print(\"Val loss\",np.mean(val_loss))\n",
    "print(\"Same dist acc\",np.mean(same_dist_acc))\n",
    "print(\"Same dist loss\",np.mean(same_dist_loss))\n",
    "\n",
    "print(\"\\nVal accuracies: \",val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful to make sure the dataset is the same when running the try twice\n",
    "def are_tuples_equal(tuple1, tuple2):\n",
    "    return [np.array_equal(arr1, arr2) for arr1, arr2 in zip(tuple1, tuple2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '67-LSTM-FT'\n",
    "model = load_try(model_name)\n",
    "\n",
    "# Loading the correct datasets\n",
    "name_person = set(np.unique(target_df['name'])).intersection(model_name.split(\"-\"))\n",
    "if len(name_person)>0:\n",
    "    name_person = next(iter(name_person)) # extract name from the string\n",
    "else:\n",
    "    name_person = None\n",
    "\n",
    "data_normal_training,data_fine_tuning,accessory = split_train_dev_test_dataset(test_dev_df_size=200,EMNIST_data_multiplier=8,random_seed=42, person_to_exclude=name_person)\n",
    "\n",
    "# unpack\n",
    "train_X, test_same_dist_X, dev_X, test_X, train_Y_categorical, test_same_dist_Y_categorical, dev_Y_categorical, test_Y_categorical = data_normal_training\n",
    "source_X_train, source_X_test, target_X_train, dev_X, test_X, source_Y_train_categorical, source_Y_test_categorical, target_Y_train_categorical, dev_Y_categorical, test_Y_categorical = data_fine_tuning\n",
    "train_Y,test_same_dist_Y,source_Y_train,source_Y_test,target_Y_train, dev_Y,test_Y = accessory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directML:\n",
    "    dev_X = dev_X.reshape(dev_X.shape[0],dev_X.shape[1],dev_X.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(dev_X, dev_Y_categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels on the development set\n",
    "y_pred = model.predict(dev_X)\n",
    "\n",
    "# Convert one-hot encoded predictions to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert one-hot encoded true labels to class labels\n",
    "y_dev_labels = np.argmax(dev_Y_categorical, axis=1)\n",
    "\n",
    "# Find indices of misclassified examples\n",
    "misclassified_indices = np.where(y_pred_labels != y_dev_labels)[0]\n",
    "well_classified_indices = np.where(y_pred_labels == y_dev_labels)[0]\n",
    "\n",
    "desired = []\n",
    "recognized = []\n",
    "for i in misclassified_indices:\n",
    "    desired.append(get_letter_from_index(y_dev_labels[i]+36))\n",
    "    recognized.append(get_letter_from_index(y_pred_labels[i]+36))\n",
    "\n",
    "print(\"number misclassified \", len(misclassified_indices))\n",
    "print(\"indices\", misclassified_indices)\n",
    "print(\"desired\", desired)\n",
    "print(\"recognized\", recognized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the mistakes and correct values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistake_number= 10\n",
    "letter = None #letter or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show misclassified letters - When the cell is rerun, it shows the next mistake\n",
    "while (letter is not None) and (mistake_number < misclassified_indices.shape[0]-1) and (get_letter_from_index(y_dev_labels[misclassified_indices[mistake_number]]+36) !=letter):\n",
    "    mistake_number+=1\n",
    "\n",
    "if np.isin(\"CNN\",network_type):\n",
    "    plt.imshow(dev_X[misclassified_indices[mistake_number]],\"gray\")\n",
    "    # plot_original(dev_X[misclassified_indices[mistake_number]])\n",
    "elif np.isin(\"LSTM\",network_type):\n",
    "    X,Y = dev_X[misclassified_indices[mistake_number]].T\n",
    "    draw_points(X,Y)\n",
    "plt.title(f'{[misclassified_indices[mistake_number]]}. Desired: {get_letter_from_index(y_dev_labels[misclassified_indices[mistake_number]]+36)} / Recognized: {get_letter_from_index(y_pred_labels[misclassified_indices[mistake_number]]+36)}')\n",
    "\n",
    "\n",
    "if(mistake_number < misclassified_indices.shape[0]-1):\n",
    "    mistake_number+=1\n",
    "else:\n",
    "    mistake_number=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_classified_number = 0\n",
    "letter = 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show misclassified letters\n",
    "while (letter is not None) and (well_classified_number < well_classified_indices.shape[0]-1) and (get_letter_from_index(y_dev_labels[well_classified_indices[well_classified_number]]+36) !=letter):\n",
    "    well_classified_number+=1\n",
    "\n",
    "if np.isin(\"CNN\",network_type):\n",
    "    plt.imshow(dev_X[well_classified_indices[well_classified_number]])\n",
    "    plot_original(dev_X[well_classified_indices[well_classified_number]])\n",
    "elif np.isin(\"LSTM\",network_type):\n",
    "    X,Y = dev_X[well_classified_indices[well_classified_number]].T\n",
    "    draw_points(X,Y)\n",
    "\n",
    "\n",
    "if(well_classified_number < well_classified_indices.shape[0]-1):\n",
    "    well_classified_number+=1\n",
    "else:\n",
    "    well_classified_number=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run all previous examples\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_dev_labels = np.argmax(dev_Y_categorical, axis=1)\n",
    "letters_labels = [get_letter_from_index(index+36) for index in range(26)]\n",
    "conf_matrix = confusion_matrix(y_dev_labels, y_pred_labels,labels=np.arange(26))\n",
    "plt.figure(figsize=(11, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues',xticklabels=letters_labels,yticklabels=letters_labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine examples of a specific set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_number = 0\n",
    "letter ='d'\n",
    "set_X = target_X_train\n",
    "set_Y = target_Y_train # do NOT provide the categorical shape\n",
    "images = set_X[set_Y==get_index_from_letter(letter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show every found examples\n",
    "if np.isin(\"CNN\",network_type):\n",
    "    plt.imshow(images[letter_number],\"gray\")\n",
    "elif np.isin(\"LSTM\",network_type):\n",
    "    X,Y = images[letter_number].T\n",
    "    draw_points(X,Y)\n",
    "\n",
    "if(letter_number < images.shape[0]-1):\n",
    "    letter_number+=1\n",
    "else:\n",
    "    letter_number=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix the results of 2 different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to do model fusion to get better overall results\n",
    "\n",
    "modelA = load_try('30-LSTM')\n",
    "modelB = load_try('14')\n",
    "\n",
    "target_index = 111\n",
    "data =  dev_X[target_index]\n",
    "\n",
    "valueA = data\n",
    "valueB = get_image_from_X(data[:32,0],target_df)\n",
    "\n",
    "draw_points(valueA[:,0],valueA[:,1])\n",
    "plt.figure()\n",
    "plt.imshow(valueB)\n",
    "\n",
    "print(\"Model A:\")\n",
    "prediction_model_a = (modelA.predict(valueA.reshape(1,250,2)))\n",
    "\n",
    "print(\"Model B:\")\n",
    "prediction_model_b = (modelB.predict(valueB.reshape(1,image_shape,image_shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation of the returned results\n",
    "print(get_letter_from_index(np.argmax(prediction_model_a)+36))\n",
    "print(get_letter_from_index(np.argmax(prediction_model_b)+36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to interpret the results\n",
    "print(get_letter_from_index(np.argmax(prediction_model_a+prediction_model_b)+36))\n",
    "[get_letter_from_index(a) for a in (np.argsort(np.max(prediction_model_a, axis=0))[::-1]+36)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Aidar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_try('34-LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aidar_df = pd.read_csv(f'{absolute_root_path}Previous_work/Personal_Experiments/Dataframes/aidar_interpol1.csv')\n",
    "total_length = DigiLeTs_X_LSTM.shape[1]\n",
    "\n",
    "if np.isin(\"CNN\",network_type):\n",
    "    aidar_df['images'] = aidar_df['images'].apply(ast.literal_eval)\n",
    "    aidar_df['images'] = aidar_df['images'].apply(np.array)\n",
    "if np.isin(\"LSTM\",network_type):\n",
    "    aidar_df['X'] = aidar_df['X'].apply(ast.literal_eval)\n",
    "    aidar_df['X'] = aidar_df['X'].apply(np.array)\n",
    "    aidar_df['Y'] = aidar_df['Y'].apply(ast.literal_eval)\n",
    "    aidar_df['Y'] = aidar_df['Y'].apply(np.array)\n",
    "\n",
    "aidar_Y = aidar_df['letter'].apply(get_index_from_letter).values # convert letters to numbers\n",
    "aidar_Y_categorical = to_categorical(aidar_Y-36, num_classes=26)\n",
    "if np.isin(\"CNN\",network_type):\n",
    "    aidar_X = np.vstack(aidar_df['images'].values).reshape((1600,image_shape,image_shape))\n",
    "if np.isin(\"LSTM\",network_type):\n",
    "    aidar_X_LSTM = np.array([np.vstack(aidar_df['X'].values), np.vstack(aidar_df['Y'].values)]).transpose(1,2,0) # reshape the data to fit the LSTM\n",
    "\n",
    "    # pad the LSTM value so that it has the same shape that the DigiLeTs values\n",
    "    aidar_X_LSTM = np.pad(aidar_X_LSTM, ((0, 0), (0, total_length -aidar_X_LSTM.shape[1]), (0, 0)), constant_values=[-10,-10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(aidar_X_LSTM,aidar_Y_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore images from aidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = np.random.randint(0,aidar_df.shape[0])\n",
    "plt.title(get_letter_from_index(aidar_Y[image_number]))\n",
    "draw_points(aidar_X_LSTM[image_number][:,0],aidar_X_LSTM[image_number][:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore mistakes on Aidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels on the development set\n",
    "y_pred = model.predict(aidar_X_LSTM)\n",
    "\n",
    "# Convert one-hot encoded predictions to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert one-hot encoded true labels to class labels\n",
    "y_dev_labels = np.argmax(aidar_Y_categorical, axis=1)\n",
    "\n",
    "# Find indices of misclassified examples\n",
    "misclassified_indices = np.where(y_pred_labels != y_dev_labels)[0]\n",
    "\n",
    "desired = []\n",
    "recognized = []\n",
    "for i in misclassified_indices:\n",
    "    desired.append(get_letter_from_index(y_dev_labels[i]+36))\n",
    "    recognized.append(get_letter_from_index(y_pred_labels[i]+36))\n",
    "\n",
    "print(\"number misclassified \", len(misclassified_indices))\n",
    "print(\"indices\", misclassified_indices)\n",
    "print(\"desired\", desired)\n",
    "print(\"recognized\", recognized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_total, total_number_letter = np.unique(aidar_df['letter'].values,return_counts=True)\n",
    "letters_mistake, mistake_number_letter= np.unique(desired,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = {letter: count for letter,count in zip(letters_total, total_number_letter)}\n",
    "mis = {letter: count for letter,count in zip(letters_mistake, mistake_number_letter)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: mis[k]/total[k]*100 if  mis[k]/total[k]*100>20 else None for k in mis.keys() & total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistake_number= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isin(\"CNN\",network_type):\n",
    "    plt.imshow(aidar_X_LSTM[misclassified_indices[mistake_number]])\n",
    "    plot_original(aidar_X_LSTM[misclassified_indices[mistake_number]])\n",
    "elif np.isin(\"LSTM\",network_type):\n",
    "    X,Y = aidar_X_LSTM[misclassified_indices[mistake_number]].T\n",
    "    draw_points(X,Y)\n",
    "plt.title(f'{[misclassified_indices[mistake_number]]}. Desired: {get_letter_from_index(y_dev_labels[misclassified_indices[mistake_number]]+36)} / Recognized: {get_letter_from_index(y_pred_labels[misclassified_indices[mistake_number]]+36)}')\n",
    "\n",
    "if(mistake_number < misclassified_indices.shape[0]-1):\n",
    "    mistake_number+=1\n",
    "else:\n",
    "    mistake_number=0\n",
    "\n",
    "plt.axis('equal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
